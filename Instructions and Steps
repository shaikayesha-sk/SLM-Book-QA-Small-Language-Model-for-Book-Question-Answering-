
### **üöÄ Updated Instructions for Running the Model **
This model allows users to ask questions based on a **PDF document** using **TF-IDF and cosine similarity** to retrieve the most relevant answer.  

---

### **1Ô∏è‚É£ Install Required Dependencies**
Before running the model, install the required Python libraries:

```bash
pip install PyPDF2 numpy joblib scikit-learn
```

---

### **2Ô∏è‚É£ Extract Text from a PDF**
Run the following code to extract text from your uploaded **PDF file**:

```python
from PyPDF2 import PdfReader

# Path to the uploaded PDF
pdf_path = "your_pdf_file.pdf"  # Replace with the actual file name

# Extract text from the PDF
reader = PdfReader(pdf_path)
text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])

# Save extracted text to a file
with open("book_text.txt", "w", encoding="utf-8") as text_file:
    text_file.write(text)

print("‚úÖ PDF text extracted and saved to 'book_text.txt'")
```

---

### **3Ô∏è‚É£ Preprocess and Chunk the Text**
Once the text is extracted, clean it and split it into smaller **chunks** for better searchability.

```python
import re
import numpy as np

# Function to clean extracted text
def clean_text(text):
    text = re.sub(r'\n+', '\n', text)  # Normalize multiple new lines
    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    return text.strip()

# Load extracted text
with open("book_text.txt", "r", encoding="utf-8") as file:
    book_text = file.read()

# Clean the text
cleaned_text = clean_text(book_text)

# Split text into chunks (500 words per chunk)
chunk_size = 500
words = cleaned_text.split()
chunks = [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

# Save chunks for retrieval
np.save("chunks.npy", np.array(chunks))

print("‚úÖ Text has been split into chunks and saved as 'chunks.npy'")
```

---

### **4Ô∏è‚É£ Create and Train the TF-IDF Model**
Now, we transform the text chunks into **TF-IDF vectors** to allow efficient question-answering.

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

# Load the saved text chunks
chunks = np.load("chunks.npy", allow_pickle=True)

# Create a TF-IDF Vectorizer
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)

# Fit and transform the text chunks into TF-IDF vectors
tfidf_matrix = vectorizer.fit_transform(chunks)

# Save the vectorized model and text chunks for retrieval
joblib.dump(vectorizer, "vectorizer.pkl")
np.save("tfidf_matrix.npy", tfidf_matrix.toarray())

print("‚úÖ TF-IDF model is now ready for question answering!")
```

---

### **5Ô∏è‚É£ Run the Question Answering Model**
Now, you can ask questions based on the **uploaded PDF**.

```python
import numpy as np
import joblib
from sklearn.metrics.pairwise import cosine_similarity

# Load the saved TF-IDF model and text chunks
vectorizer = joblib.load("vectorizer.pkl")
tfidf_matrix = np.load("tfidf_matrix.npy")
chunks = np.load("chunks.npy", allow_pickle=True)

print("üí¨ Type your questions below. Type 'exit' to stop.\n")

while True:
    # Ask a question
    question = input("Ask a question: ")

    # Stop if the user types "exit"
    if question.lower() == "exit":
        print("üö™ Exiting the Q&A system. Have a great day! üòä")
        break

    # Convert the question into a TF-IDF vector
    question_vector = vectorizer.transform([question])

    # Compute cosine similarity between the question and book chunks
    similarities = cosine_similarity(question_vector, tfidf_matrix)

    # Find the most relevant chunk
    best_match_idx = np.argmax(similarities)

    # Retrieve the most relevant text from the book
    best_answer = chunks[best_match_idx]

    print("\nüìñ Answer from the book:\n")
    print(best_answer)
    print("\n" + "-"*80 + "\n")
```

---

### **6Ô∏è‚É£ Sample Input & Expected Output**
Once the model is running, here‚Äôs an example interaction:

#### **Example Input**
```bash
Ask a question: What is a linked list?
```

#### **Expected Output**
```bash
üìñ Answer from the book:
Linked lists are data structures that consist of nodes, where each node contains data and a reference to the next node.
```

---

## **üìå How to Upload to GitHub**
### **1Ô∏è‚É£ Initialize a Git Repository**
Run the following command inside your project folder:

```bash
git init
```

### **2Ô∏è‚É£ Add the Files**
```bash
git add .
```

### **3Ô∏è‚É£ Commit the Changes**
```bash
git commit -m "Added TF-IDF Question Answering Model"
```

---


{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58638f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m964.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c47e16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Structures and\\nAlgorithms in Python\\nMichael T. Goodrich\\nDepartment of Computer Science\\nUniversity of California, Irvine\\nRoberto Tamassia\\nDepartment of Computer Science\\nBrown University\\nMichael H. Goldwasser\\nDepartment of Mathematics and Computer Science\\nSaint Louis University\\n\\nVP & PUBLISHER Don Fowley\\nEXECUTIVE EDITOR Beth Lang Golub\\nEDITORIAL PROGRAM ASSISTANT Katherine WillisMARKETING MANAGER Christopher RuelDESIGNER Kenji NgiengSENIOR PRODUCTION MANAGER Janis SooASSOCIATE PRODUCTION MANAGER Joyce Poh\\nThis book was set in L aTEX by the authors. Printed and bound by Courier Westford.\\nThe cover was printed by Courier Westford.\\nThis book is printed on acid free paper. Founded in 1807, John Wiley & Sons, Inc. has been a valued source of knowledge and understanding for \\nmore than 200 years, helping people around the world meet their needs and fulï¬  ll their aspirations. Our company is built on a foundation of principles that include responsibility to the communities we serve and whe'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Path to the uploaded PDF\n",
    "pdf_path = \"Data Structures and Algorithms in Python.pdf\"\n",
    "\n",
    "# Extract text from the PDF\n",
    "reader = PdfReader(pdf_path)\n",
    "text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "# Previewing the first 1000 characters to check extraction quality\n",
    "text[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1095ae27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Structures and Algorithms in Python Michael T. Goodrich Department of Computer Science University of California, Irvine Roberto Tamassia Department of Computer Science Brown University Michael H. Goldwasser Department of Mathematics and Computer Science Saint Louis University VP & PUBLISHER Don Fowley EXECUTIVE EDITOR Beth Lang Golub EDITORIAL PROGRAM ASSISTANT Katherine WillisMARKETING MANAGER Christopher RuelDESIGNER Kenji NgiengSENIOR PRODUCTION MANAGER Janis SooASSOCIATE PRODUCTION MANAGER Joyce Poh This book was set in L aTEX by the authors. Printed and bound by Courier Westford. The cover was printed by Courier Westford. This book is printed on acid free paper. Founded in 1807, John Wiley & Sons, Inc. has been a valued source of knowledge and understanding for more than 200 years, helping people around the world meet their needs and ful ll their aspirations. Our company is built on a foundation of principles that include responsibility to the communities we serve and where we live and work. In 2008, we launched a Corporate Citizenship Initiative, a global effort to address the environmental, social, economic, and ethical challenges we face in our business. Among the issues we are addressing are carbon impact, paper speci cations and procurement, ethical conduct within our business and among our vendors, and community and charitable support. For more information, please visit our website: www.wiley.com/go/citizenship. Copyright 2013 John Wiley & Sons, Inc. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted under Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc. 222 Rosewood Drive, Danvers, MA 01923, website www.copyright.com. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030-5774, (201)748-6011, fax (201)748-6008, website http://www.wiley.com/go/permissions. Evaluation copies are provided to quali ed academics and professionals for review purposes only, for use in their courses during the next academic year. These copies are licensed and may not be sold or transferred to a third party. Upon completion of the review period, please return the evaluation copy to Wiley. Return instructions and a free of charge return mailing label are available at www.wiley.com/go/returnlabel. If you have chosen to adopt this textbook for use in your course, please accept this book as your complimentary desk copy. Outside of the United States, please contact your local sales representative. Printed in the United States of America10 9 8 7 6 5 4 3 2 1 To Karen, Paul, Anna, and Jack Michael T. Goodrich To Isabel Roberto Tamassia To Susan, Calista, and Maya Michael H. Goldwasser Preface The design and analysis of ef cient data structures has long been recognized as a vital subject in computing and is part of the core curriculum of computer scienceand computer engineering undergraduate degrees.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean extracted text\n",
    "def clean_text(text):\n",
    "    # Remove extra spaces, new lines, and special characters\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Normalize multiple new lines\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Clean the extracted text\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "# Splitting the text into manageable chunks (500 words per chunk for efficient retrieval)\n",
    "words = cleaned_text.split()\n",
    "chunk_size = 500  # Approximate word count per chunk\n",
    "chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# Display the first chunk as a preview\n",
    "chunks[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820f39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m363.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.6.0-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.11.4)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.9.0)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.7.4)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp311-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m408.9/408.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, sympy, safetensors, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed huggingface-hub-0.28.1 safetensors-0.5.2 sentence-transformers-3.4.1 sympy-1.13.1 tokenizers-0.21.0 torch-2.6.0 transformers-4.48.2 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f04414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /opt/anaconda3/lib/python3.11/site-packages (3.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdb7b780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PDF text extracted and saved to 'book_text.txt'\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_path = \"Data Structures and Algorithms in Python.pdf\"\n",
    "\n",
    "with open(pdf_path, \"rb\") as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "# Save extracted text to a file\n",
    "with open(\"book_text.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(text)\n",
    "\n",
    "print(\"âœ… PDF text extracted and saved to 'book_text.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c870dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text has been split into chunks and saved as 'chunks.npy'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the extracted text\n",
    "with open(\"book_text.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    book_text = file.read()\n",
    "\n",
    "# Split text into chunks\n",
    "chunk_size = 500  # Adjust as needed\n",
    "words = book_text.split()\n",
    "chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# Save chunks for retrieval\n",
    "np.save(\"chunks.npy\", np.array(chunks))\n",
    "\n",
    "print(\"âœ… Text has been split into chunks and saved as 'chunks.npy'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb120420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TF-IDF model is now ready for question answering!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load the saved text chunks\n",
    "chunks = np.load(\"chunks.npy\", allow_pickle=True)\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit and transform the text chunks into TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(chunks)\n",
    "\n",
    "# Save the vectorized model and text chunks for retrieval\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "np.save(\"tfidf_matrix.npy\", tfidf_matrix.toarray())\n",
    "\n",
    "print(\"âœ… TF-IDF model is now ready for question answering!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df58d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ Type your questions below. Type 'exit' to stop.\n",
      "\n",
      "Ask a question: How does binary search work?\n",
      "\n",
      "ğŸ“– Answer from the book:\n",
      "\n",
      "Code Fragment 8.15, and an example of an inordertraversal is portrayed in Figure 8.18. Algorithm inorder(p) : ifphas a left child lcthen inorder(lc) {recursively traverse the left subtree of p} perform the â€œvisitâ€ action for position p ifphas a right child rcthen inorder(rc) {recursively traverse the right subtree of p} Code Fragment 8.15: Algorithm inorder for performing an inorder traversal of a subtree rooted at position pof a binary tree. 3 1 9 5 4 7+ 3 2 âˆ’ 3 âˆ’Ã— + Ã— 6/ +âˆ’ Figure 8.18: Inorder traversal of a binary tree. The inorder traversal algorithm has several important applications. When using a binary tree to represent an arithmetic expression, as in Figure 8.18, the inorder traversal visits positions in a consistent order with the standard representation of the expression, as in 3 +1Ã—3/9âˆ’5+2...(albeit without parentheses). 332 Chapter 8. Trees Binary Search Trees An important application of the inorder traversal algorithm arises when we store an ordered sequence of elements in a binary tree, deï¬ning a structure we call a binary search tree .L e t Sbe a set whose unique elements have an order relation. For example, Scould be a set of integers. A binary search tree for Sis a binary tree T such that, for each position pofT: â€¢Position pstores an element of S, denoted as e(p). â€¢Elements stored in the left subtree of p(if any) are less than e(p). â€¢Elements stored in the right subtree of p(if any) are greater than e(p). An example of a binary search tree is shown in Figure 8.19. The above properties assure that an inorder traversal of a binary search tree Tvisits the elements in nondecreasing order. 362531 42 1262 7558 90 Figure 8.19: A binary search tree storing integers. The solid path is traversed when searching (successfully) for 36. The dashed path is traversed when searching (un- successfully) for 70. We can use a binary search tree Tfor set Sto ï¬nd whether a given search value vis in S, by traversing a path down the tree T, starting at the root. At each internal position pencountered, we compare our search value vwith the element e(p)stored at p.I f v<e(p), then the search continues in the left subtree of p. Ifv=e(p), then the search terminates successfully. If v>e(p), then the search continues in the right subtree of p. Finally, if we reach an empty subtree, the search terminates unsuccessfully. In other words, a binary search tree can be viewed as abinary decision tree (recall Example 8.6), where the question asked at each internalnode is whether the element at that node is less than, equal to, or larger than the element being searched for. We illustrate several examples of the search operation in Figure 8.19. Note that the running time of searching in a binary search tree Tis proportional to the height of T. Recall from Proposition 8.8 that the height of a binary tree with nnodes can be as small as log (n+1)âˆ’1 or as large as nâˆ’1. Thus, binary\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ask a question: Explain recursion with an example.\n",
      "\n",
      "ğŸ“– Answer from the book:\n",
      "\n",
      "recursive call may start two others, we call this a binary recursion . â€¢If a recursive call may start three or more others, this is multiple recursion . 4.4.1 Linear Recursion If a recursive function is designed so that each invocation of the body makes at most one new recursive call, this is know as linear recursion . Of the recursions we have seen so far, the implementation of the factorial function (Section 4.1.1) andthegood ï¬bonacci function (Section 4.3) are clear examples of linear recursion. More interestingly, the binary search algorithm (Section 4.1.3) is also an example oflinear recursion , despite the â€œbinaryâ€ terminology in the name. The code for binary search (Code Fragment 4.3) includes a case analysis with two branches that lead to recursive calls, but only one of those calls can be reached during a particularexecution of the body. A consequence of the deï¬nition of linear recursion is that any recursion trace will appear as a single sequence of calls, as we originally portrayed for the factorialfunction in Figure 4.1 of Section 4.1.1. Note that the linear recursion terminol- ogy reï¬‚ects the structure of the recursion trace, not the asymptotic analysis of the running time; for example, we have seen that binary search runs in O(logn)time. Summing the Elements of a Sequence Recursively Linear recursion can be a useful tool for processing a data sequence, such as a Python list. Suppose, for example, that we want to compute the sum of a sequence,S,o fnintegers. We can solve this summation problem using linear recursion by observing that the sum of all nintegers in Sis trivially 0, if n=0, and otherwise that it is the sum of the ï¬rst nâˆ’1i n t e g e r si n Splus the last element in S.( S e e Figure 4.9.) 4362893285172835 701234 6789 1 0 1 1 1 2 1 3 1 4 1 5 Figure 4.9: Computing the sum of a sequence recursively, by adding the last number to the sum of the ï¬rst nâˆ’1. 170 Chapter 4. Recursion A recursive algorithm for computing the sum of a sequence of numbers based on this intuition is implemented in Code Fragment 4.9. 1deflinear sum(S, n): 2â€â€â€Return the sum of the ï¬rst n numbers of sequence S.â€â€â€ 3ifn= =0 : 4 return 0 5else: 6 return linear sum(S, n âˆ’1) + S[n âˆ’1] Code Fragment 4.9: Summing the elements of a sequence using linear recursion. A recursion trace of the linear sum function for a small example is given in Figure 4.10. For an input of size n,t h elinear sum algorithm makes n+1 function calls. Hence, it will take O(n)time, because it spends a constant amount of time performing the nonrecursive part of each call. Moreover, we can also see that the memory space used by the algorithm (in addition to the sequence S)i sa l s o O(n),a s we use a constant amount of memory space for each of the n+1 activation records in the trace at the\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ask a question: What are the key differences between stacks and queues?\n",
      "\n",
      "ğŸ“– Answer from the book:\n",
      "\n",
      "i o n ..................... 174 4 . 4 . 3 M u l t i p l e R e c u r s i o n .................... 175 4.5 Designing Recursive Algorithms ................ 177 4.6 Eliminating Tail Recursion ................... 178 4.7 Exercises ............................. 180 5 Array-Based Sequences 183 5.1 Pythonâ€™s Sequence Types .................... 184 5.2 Low-Level Arrays ......................... 185 5 . 2 . 1 R e f e r e n t i a l A r r a y s..................... 187 5 . 2 . 2 C o m p a c tA r r a y s i n P y t h o n ................ 190 5.3 Dynamic Arrays and Amortization ............... 192 5 . 3 . 1 I m p l e m e n t i n ga D y n a m i c A r r a y.............. 195 5 . 3 . 2 A m o r t i z e dA n a l y s i s o f D y n a m i c A r r a y s.......... 197 5 . 3 . 3 P y t h o n â€™ sL i s t C l a s s .................... 201 5.4 Eï¬ƒciency of Pythonâ€™s Sequence Types ............ 202 5 . 4 . 1 P y t h o n â€™ sL i s t a n d T u p l e C l a s s e s ............. 202 5 . 4 . 2 P y t h o n â€™ sS t r i n g C l a s s................... 208 5.5 Using Array-Based Sequences ................. 210 5 . 5 . 1 S t o r i n g H i g hS c o r e sf o ra G a m e ............. 210 5 . 5 . 2 S o r t i n g a S e q u e n c e .................... 214 5 . 5 . 3 S i m p l e C r y p t o g r a p h y ................... 216 5.6 Multidimensional Data Sets .................. 219 5.7 Exercises ............................. 224 6 Stacks, Queues, and Deques 228 6.1 Stacks ............................... 229 6 . 1 . 1 T h e S t a c k A b s t r a c t D a t a T y p e.............. 230 6 . 1 . 2 S i m p l e A r r a y - B a s e d S t a c k I m p l e m e n t a t i o n........ 231 6 . 1 . 3 R e v e r s i n g D a t a U s i n g a S t a c k .............. 235 6 . 1 . 4 M a t c h i n g P a r e n t h e s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ask a question: exit\n",
      "ğŸšª Exiting the Q&A system. Have a great day! ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the saved TF-IDF model and text chunks\n",
    "vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "tfidf_matrix = np.load(\"tfidf_matrix.npy\")\n",
    "chunks = np.load(\"chunks.npy\", allow_pickle=True)\n",
    "\n",
    "print(\"ğŸ’¬ Type your questions below. Type 'exit' to stop.\\n\")\n",
    "\n",
    "while True:\n",
    "    # Ask a question\n",
    "    question = input(\"Ask a question: \")\n",
    "\n",
    "    # Stop if the user types \"exit\"\n",
    "    if question.lower() == \"exit\":\n",
    "        print(\"ğŸšª Exiting the Q&A system. Have a great day! ğŸ˜Š\")\n",
    "        break\n",
    "\n",
    "    # Convert the question into a TF-IDF vector\n",
    "    question_vector = vectorizer.transform([question])\n",
    "\n",
    "    # Compute cosine similarity between the question and book chunks\n",
    "    similarities = cosine_similarity(question_vector, tfidf_matrix)\n",
    "\n",
    "    # Find the most relevant chunk\n",
    "    best_match_idx = np.argmax(similarities)\n",
    "\n",
    "    # Retrieve the most relevant text from the book\n",
    "    best_answer = chunks[best_match_idx]\n",
    "\n",
    "    print(\"\\nğŸ“– Answer from the book:\\n\")\n",
    "    print(best_answer)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff7ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
